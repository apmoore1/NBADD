Char Model
Metrics: {
  "peak_cpu_memory_MB": 3175.252,
  "peak_gpu_0_memory_MB": 2307,
  "training_duration": "00:16:56",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.8262976020920404,
  "training_loss": 0.480968435275096,
  "training_cpu_memory_MB": 3175.252,
  "training_gpu_0_memory_MB": 2258,
  "validation_accuracy": 0.825561797752809,
  "validation_loss": 0.5018170864326869,
  "best_epoch": 10,
  "best_validation_accuracy": 0.826123595505618,
  "best_validation_loss": 0.49880193548645085,
  "test_accuracy": 0.8152092675635276,
  "test_loss": 0.5185614877894743
}

Word Model
Metrics: {
  "peak_cpu_memory_MB": 3260.868,
  "peak_gpu_0_memory_MB": 2401,
  "training_duration": "00:16:54",
  "training_start_epoch": 0,
  "training_epochs": 7,
  "epoch": 7,
  "training_accuracy": 0.8581802050012842,
  "training_loss": 0.40677456731421313,
  "training_cpu_memory_MB": 3260.868,
  "training_gpu_0_memory_MB": 2399,
  "validation_accuracy": 0.8222846441947566,
  "validation_loss": 0.5486910747598388,
  "best_epoch": 3,
  "best_validation_accuracy": 0.8300561797752809,
  "best_validation_loss": 0.5044355646534238,
  "test_accuracy": 0.8242713004484304,
  "test_loss": 0.5203503264642474
}

Word Char model
Metrics: {
  "peak_cpu_memory_MB": 3485.432,
  "peak_gpu_0_memory_MB": 3632,
  "training_duration": "00:18:37",
  "training_start_epoch": 0,
  "training_epochs": 6,
  "epoch": 6,
  "training_accuracy": 0.8548646944827103,
  "training_loss": 0.4124519728767057,
  "training_cpu_memory_MB": 3485.432,
  "training_gpu_0_memory_MB": 3631,
  "validation_accuracy": 0.8484082397003745,
  "validation_loss": 0.4766507111489773,
  "best_epoch": 2,
  "best_validation_accuracy": 0.8473782771535581,
  "best_validation_loss": 0.4606484800443321,
  "test_accuracy": 0.8425822122571002,
  "test_loss": 0.46880202019837364
}

Word Attention model
Metrics: {
  "peak_cpu_memory_MB": 3259.992,
  "peak_gpu_0_memory_MB": 2343,
  "training_duration": "00:15:23",
  "training_start_epoch": 0,
  "training_epochs": 6,
  "epoch": 6,
  "training_accuracy": 0.8458054122206916,
  "training_loss": 0.44488961967372787,
  "training_cpu_memory_MB": 3259.94,
  "training_gpu_0_memory_MB": 2343,
  "validation_accuracy": 0.8341760299625468,
  "validation_loss": 0.5081306721591307,
  "best_epoch": 2,
  "best_validation_accuracy": 0.8405430711610486,
  "best_validation_loss": 0.47112042630860906,
  "test_accuracy": 0.8341741405082213,
  "test_loss": 0.49234127909389896
}

Word Char Attention model
Metrics: {
  "peak_cpu_memory_MB": 3487.528,
  "peak_gpu_0_memory_MB": 3642,
  "training_duration": "00:16:36",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8425716220318009,
  "training_loss": 0.4535125746243161,
  "training_cpu_memory_MB": 3487.528,
  "training_gpu_0_memory_MB": 3642,
  "validation_accuracy": 0.8397940074906367,
  "validation_loss": 0.48537447693908287,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8502808988764045,
  "best_validation_loss": 0.4494520527293939,
  "test_accuracy": 0.8403400597907325,
  "test_loss": 0.4676147585484519
} 

Word Char Attention model with code switching lexicon regularisation of 0.05
Metrics: {
  "peak_cpu_memory_MB": 3539.216,
  "peak_gpu_0_memory_MB": 3758,
  "training_duration": "00:16:55",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8464241518597212,
  "training_loss": 2.0251926582029696,
  "training_cpu_memory_MB": 3539.08,
  "training_gpu_0_memory_MB": 3758,
  "validation_accuracy": 0.8351123595505618,
  "validation_loss": 2.044238454567458,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8316479400749064,
  "best_validation_loss": 2.0417743260988934,
  "test_accuracy": 0.8210949177877429,
  "test_loss": 2.079232650016671
}
Word Char Attention model with code switching lexicon regularisation of 0.01
 Metrics: {
  "peak_cpu_memory_MB": 3534.72,
  "peak_gpu_0_memory_MB": 3817,
  "training_duration": "00:16:49",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8453968105722758,
  "training_loss": 0.7605353492295782,
  "training_cpu_memory_MB": 3534.72,
  "training_gpu_0_memory_MB": 3817,
  "validation_accuracy": 0.8258426966292135,
  "validation_loss": 0.8416300346394499,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8372659176029963,
  "best_validation_loss": 0.8018568815763839,
  "test_accuracy": 0.8309977578475336,
  "test_loss": 0.821608951198521
}
Word Char Attention model with code switching lexicon regularisation of 0.005
Metrics: {
  "peak_cpu_memory_MB": 3533.984,
  "peak_gpu_0_memory_MB": 3729,
  "training_duration": "00:16:39",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8455835998972658,
  "training_loss": 0.6034149149148198,
  "training_cpu_memory_MB": 3533.984,
  "training_gpu_0_memory_MB": 3729,
  "validation_accuracy": 0.8289325842696629,
  "validation_loss": 0.6847463910005049,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8372659176029963,
  "best_validation_loss": 0.6492661666013524,
  "test_accuracy": 0.8288490284005979,
  "test_loss": 0.6694889581915158
}
Word Char Attention model with code switching lexicon regularisation of 0.001
Metrics: {
  "peak_cpu_memory_MB": 3537.008,
  "peak_gpu_0_memory_MB": 3754,
  "training_duration": "00:16:40",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8460855962081767,
  "training_loss": 0.47686283798595397,
  "training_cpu_memory_MB": 3537.008,
  "training_gpu_0_memory_MB": 3754,
  "validation_accuracy": 0.8364232209737827,
  "validation_loss": 0.5291240949875223,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8460674157303371,
  "best_validation_loss": 0.5013139327307661,
  "test_accuracy": 0.8393124065769806,
  "test_loss": 0.5209303150870906
}
Word Char Attention model with code switching lexicon regularisation of 0.0001
Metrics: {
  "peak_cpu_memory_MB": 3533.788,
  "peak_gpu_0_memory_MB": 4055,
  "training_duration": "00:22:07",
  "training_start_epoch": 0,
  "training_epochs": 7,
  "epoch": 7,
  "training_accuracy": 0.8541759088468094,
  "training_loss": 0.42157505501477766,
  "training_cpu_memory_MB": 3533.788,
  "training_gpu_0_memory_MB": 3763,
  "validation_accuracy": 0.8348314606741573,
  "validation_loss": 0.5170393015988572,
  "best_epoch": 3,
  "best_validation_accuracy": 0.8402621722846442,
  "best_validation_loss": 0.4789790613521002,
  "test_accuracy": 0.8304372197309418,
  "test_loss": 0.49989160436080465
}

Word Attention model with code switching lexicon regularisation of 0.001
Metrics: {
  "peak_cpu_memory_MB": 3304.864,
  "peak_gpu_0_memory_MB": 2497,
  "training_duration": "00:11:17",
  "training_start_epoch": 0,
  "training_epochs": 4,
  "epoch": 4,
  "training_accuracy": 0.83430619440099,
  "training_loss": 0.5085391023731695,
  "training_cpu_memory_MB": 3304.848,
  "training_gpu_0_memory_MB": 2488,
  "validation_accuracy": 0.8264044943820225,
  "validation_loss": 0.5645391666915959,
  "best_epoch": 0,
  "best_validation_accuracy": 0.8382022471910112,
  "best_validation_loss": 0.5184789649963736,
  "test_accuracy": 0.8353886397608371,
  "test_loss": 0.5328422677383494
}

Word Char Attention model with bivalency lexicon regularisation of 0.05:
Metrics: {
  "peak_cpu_memory_MB": 3535.008,
  "peak_gpu_0_memory_MB": 3909,
  "training_duration": "00:16:40",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8462023395362955,
  "training_loss": 2.01285112840407,
  "training_cpu_memory_MB": 3535.008,
  "training_gpu_0_memory_MB": 3909,
  "validation_accuracy": 0.8303370786516854,
  "validation_loss": 0.531808285823393,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8352059925093633,
  "best_validation_loss": 0.49878052008633844,
  "test_accuracy": 0.8247384155455905,
  "test_loss": 0.5104362439753404
}

Word Char Attention model with bivalency lexicon regularisation of 0.01:
Metrics: {
  "peak_cpu_memory_MB": 3535.024,
  "peak_gpu_0_memory_MB": 3917,
  "training_duration": "00:16:49",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8476266081393449,
  "training_loss": 0.7574893367192529,
  "training_cpu_memory_MB": 3535.024,
  "training_gpu_0_memory_MB": 3888,
  "validation_accuracy": 0.8341760299625468,
  "validation_loss": 0.5250253819910708,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8396067415730337,
  "best_validation_loss": 0.49042914999637777,
  "test_accuracy": 0.8317451420029895,
  "test_loss": 0.49671686778762447
}

Word Char Attention model with bivalency lexicon regularisation of 0.005:
Metrics: {
  "peak_cpu_memory_MB": 3535.788,
  "peak_gpu_0_memory_MB": 3989,
  "training_duration": "00:16:42",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8478834434612061,
  "training_loss": 0.5983103543275053,
  "training_cpu_memory_MB": 3535.592,
  "training_gpu_0_memory_MB": 3988,
  "validation_accuracy": 0.8352059925093633,
  "validation_loss": 0.5171957643936851,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8354868913857678,
  "best_validation_loss": 0.49247527445950906,
  "test_accuracy": 0.8267002989536621,
  "test_loss": 0.5143602060293084
}

Word Char Attention model with bivalency lexicon regularisation of 0.001:
 Metrics: {
  "peak_cpu_memory_MB": 3538.58,
  "peak_gpu_0_memory_MB": 3989,
  "training_duration": "00:16:43",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8445562586098204,
  "training_loss": 0.4791133262809863,
  "training_cpu_memory_MB": 3538.448,
  "training_gpu_0_memory_MB": 3989,
  "validation_accuracy": 0.8280898876404494,
  "validation_loss": 0.5351940628118858,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8413857677902622,
  "best_validation_loss": 0.4798222964261463,
  "test_accuracy": 0.8262331838565022,
  "test_loss": 0.507497094927439
}
Word Char Attention model with bivalency lexicon regularisation of 0.0001
Metrics: {
  "peak_cpu_memory_MB": 3537.928,
  "peak_gpu_0_memory_MB": 4583,
  "training_duration": "00:30:31",
  "training_start_epoch": 0,
  "training_epochs": 10,
  "epoch": 10,
  "training_accuracy": 0.8693408671694413,
  "training_loss": 0.3815700642369356,
  "training_cpu_memory_MB": 3537.928,
  "training_gpu_0_memory_MB": 4583,
  "validation_accuracy": 0.8262172284644195,
  "validation_loss": 0.5588262422891435,
  "best_epoch": 6,
  "best_validation_accuracy": 0.8440074906367041,
  "best_validation_loss": 0.4763079051396804,
  "test_accuracy": 0.8307174887892377,
  "test_loss": 0.5049760430161633
}
Word Char Attention model with bivalency and code switching lexicon regularisation of 0.001
Metrics: {
  "peak_cpu_memory_MB": 3581.544,
  "peak_gpu_0_memory_MB": 3704,
  "training_duration": "00:27:54",
  "training_start_epoch": 0,
  "training_epochs": 9,
  "epoch": 9,
  "training_accuracy": 0.8648462490368676,
  "training_loss": 0.4513321070805565,
  "training_cpu_memory_MB": 3581.404,
  "training_gpu_0_memory_MB": 3704,
  "validation_accuracy": 0.8283707865168539,
  "validation_loss": 0.5534862403473454,
  "best_epoch": 5,
  "best_validation_accuracy": 0.8463483146067415,
  "best_validation_loss": 0.47183350980014144,
  "test_accuracy": 0.8313714499252616,
  "test_loss": 0.5014374330639839
}