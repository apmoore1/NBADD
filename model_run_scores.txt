Char Model
Metrics: {
  "peak_cpu_memory_MB": 3175.252,
  "peak_gpu_0_memory_MB": 2307,
  "training_duration": "00:16:56",
  "training_start_epoch": 0,
  "training_epochs": 14,
  "epoch": 14,
  "training_accuracy": 0.8262976020920404,
  "training_loss": 0.480968435275096,
  "training_cpu_memory_MB": 3175.252,
  "training_gpu_0_memory_MB": 2258,
  "validation_accuracy": 0.825561797752809,
  "validation_loss": 0.5018170864326869,
  "best_epoch": 10,
  "best_validation_accuracy": 0.826123595505618,
  "best_validation_loss": 0.49880193548645085,
  "test_accuracy": 0.8152092675635276,
  "test_loss": 0.5185614877894743
}

Word Model
Metrics: {
  "peak_cpu_memory_MB": 3260.868,
  "peak_gpu_0_memory_MB": 2401,
  "training_duration": "00:16:54",
  "training_start_epoch": 0,
  "training_epochs": 7,
  "epoch": 7,
  "training_accuracy": 0.8581802050012842,
  "training_loss": 0.40677456731421313,
  "training_cpu_memory_MB": 3260.868,
  "training_gpu_0_memory_MB": 2399,
  "validation_accuracy": 0.8222846441947566,
  "validation_loss": 0.5486910747598388,
  "best_epoch": 3,
  "best_validation_accuracy": 0.8300561797752809,
  "best_validation_loss": 0.5044355646534238,
  "test_accuracy": 0.8242713004484304,
  "test_loss": 0.5203503264642474
}

Word Char model
Metrics: {
  "peak_cpu_memory_MB": 3485.432,
  "peak_gpu_0_memory_MB": 3632,
  "training_duration": "00:18:37",
  "training_start_epoch": 0,
  "training_epochs": 6,
  "epoch": 6,
  "training_accuracy": 0.8548646944827103,
  "training_loss": 0.4124519728767057,
  "training_cpu_memory_MB": 3485.432,
  "training_gpu_0_memory_MB": 3631,
  "validation_accuracy": 0.8484082397003745,
  "validation_loss": 0.4766507111489773,
  "best_epoch": 2,
  "best_validation_accuracy": 0.8473782771535581,
  "best_validation_loss": 0.4606484800443321,
  "test_accuracy": 0.8425822122571002,
  "test_loss": 0.46880202019837364
}

Word Attention model
Metrics: {
  "peak_cpu_memory_MB": 3259.992,
  "peak_gpu_0_memory_MB": 2343,
  "training_duration": "00:15:23",
  "training_start_epoch": 0,
  "training_epochs": 6,
  "epoch": 6,
  "training_accuracy": 0.8458054122206916,
  "training_loss": 0.44488961967372787,
  "training_cpu_memory_MB": 3259.94,
  "training_gpu_0_memory_MB": 2343,
  "validation_accuracy": 0.8341760299625468,
  "validation_loss": 0.5081306721591307,
  "best_epoch": 2,
  "best_validation_accuracy": 0.8405430711610486,
  "best_validation_loss": 0.47112042630860906,
  "test_accuracy": 0.8341741405082213,
  "test_loss": 0.49234127909389896
}

Word Char Attention model
Metrics: {
  "peak_cpu_memory_MB": 3487.528,
  "peak_gpu_0_memory_MB": 3642,
  "training_duration": "00:16:36",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8425716220318009,
  "training_loss": 0.4535125746243161,
  "training_cpu_memory_MB": 3487.528,
  "training_gpu_0_memory_MB": 3642,
  "validation_accuracy": 0.8397940074906367,
  "validation_loss": 0.48537447693908287,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8502808988764045,
  "best_validation_loss": 0.4494520527293939,
  "test_accuracy": 0.8403400597907325,
  "test_loss": 0.4676147585484519
} 


Word Char Attention model with code switching lexicon regularisation of 0.001
Metrics: {
  "peak_cpu_memory_MB": 3536.28,
  "peak_gpu_0_memory_MB": 3804,
  "training_duration": "00:16:47",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8460272245441173,
  "training_loss": 0.47636842984548916,
  "training_cpu_memory_MB": 3536.28,
  "training_gpu_0_memory_MB": 3804,
  "validation_accuracy": 0.8345505617977528,
  "validation_loss": 0.5073876928783463,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8442883895131086,
  "best_validation_loss": 0.4736812498844312,
  "test_accuracy": 0.836416292974589,
  "test_loss": 0.4952441009345339
}


Word Char Attention model with bivalency lexicon regularisation of 0.01:
Metrics: {
  "peak_cpu_memory_MB": 3535.024,
  "peak_gpu_0_memory_MB": 3917,
  "training_duration": "00:16:49",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
  "training_accuracy": 0.8476266081393449,
  "training_loss": 0.7574893367192529,
  "training_cpu_memory_MB": 3535.024,
  "training_gpu_0_memory_MB": 3888,
  "validation_accuracy": 0.8341760299625468,
  "validation_loss": 0.5250253819910708,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8396067415730337,
  "best_validation_loss": 0.49042914999637777,
  "test_accuracy": 0.8317451420029895,
  "test_loss": 0.49671686778762447
}

Word Char Attention model with (Strong) bivalency lexicon regularisation of 0.01:
 Metrics: {
  "peak_cpu_memory_MB": 3534.08,
  "peak_gpu_0_memory_MB": 4116,
  "training_duration": "00:22:19",
  "training_start_epoch": 0,
  "training_epochs": 7,
  "epoch": 7,
  "training_accuracy": 0.8573396530388289,
  "training_loss": 0.7268094726527499,
  "training_cpu_memory_MB": 3534.08,
  "training_gpu_0_memory_MB": 3824,
  "validation_accuracy": 0.8176029962546817,
  "validation_loss": 0.5604245470847912,
  "best_epoch": 3,
  "best_validation_accuracy": 0.8314606741573034,
  "best_validation_loss": 0.510465194075229,
  "test_accuracy": 0.8189461883408071,
  "test_loss": 0.5269110059560235
}

Word Char Attention model with bivalency lexicon regularisation of 0.001:
 Metrics: {
  "peak_cpu_memory_MB": 3538.58,
  "peak_gpu_0_memory_MB": 3989,
  "training_duration": "00:16:43",
  "training_start_epoch": 0,
  "training_epochs": 5,
  "epoch": 5,
"training_accuracy": 0.8448714655957412,
  "training_loss": 0.477464135606809,
  "training_cpu_memory_MB": 3537.468,
  "training_gpu_0_memory_MB": 3834,
  "validation_accuracy": 0.8323970037453183,
  "validation_loss": 0.5199300236330775,
  "best_epoch": 1,
  "best_validation_accuracy": 0.8406367041198501,
  "best_validation_loss": 0.48190429525639483,
  "test_accuracy": 0.8300635276532138,
  "test_loss": 0.5008896886635182
}
